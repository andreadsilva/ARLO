{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mushroom_rl.algorithms.value import DQN\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.environments import CartPole\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.utils.dataset import compute_metrics\n",
    "from mushroom_rl.utils.parameters import LinearParameter, Parameter\n",
    "\n",
    "mdp = CartPole(gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 0\n",
    "if w :\n",
    "    w+1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch(epoch):\n",
    "    print('################################################################')\n",
    "    print('Epoch: ', epoch)\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "def get_stats(dataset):\n",
    "    score = compute_metrics(dataset)\n",
    "    print(('min_reward: %f, max_reward: %f, mean_reward: %f,'\n",
    "          ' games_completed: %d' % score))\n",
    "\n",
    "    return score\n",
    "\n",
    "scores = list()\n",
    "\n",
    "train_frequency = 1\n",
    "evaluation_frequency = 7500\n",
    "target_update_frequency = 500\n",
    "initial_replay_size = 5000\n",
    "max_replay_size = 50000\n",
    "test_samples = 500\n",
    "max_steps = 52500\n",
    "\n",
    "\n",
    "# Policy\n",
    "epsilon = LinearParameter(value=1.,\n",
    "                          threshold_value=.1,\n",
    "                          n=10000)\n",
    "epsilon_test = Parameter(value=.05)\n",
    "epsilon_random = Parameter(value=1)\n",
    "pi = EpsGreedy(epsilon=epsilon_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.utils.callbacks.collect_dataset import CollectDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self.hl0 = nn.Linear(n_input, 16)\n",
    "        self.hl1 = nn.Linear(16, 16)\n",
    "        self.hl2 = nn.Linear(16, n_output)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.hl0.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.hl1.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.hl2.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        h = F.relu(self.hl0(state.float()))\n",
    "        h = F.relu(self.hl1(h))\n",
    "        q = self.hl2(h)\n",
    "\n",
    "        if action is None:\n",
    "            return q\n",
    "        else:\n",
    "            q_acted = torch.squeeze(q.gather(1, action.long()))            \n",
    "            return q_acted\n",
    "\n",
    "network =Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer=dict()\n",
    "optimizer['class'] = torch.optim.Adam\n",
    "optimizer['params'] = dict(lr=1e-3)\n",
    "\n",
    "# Approximator\n",
    "approximator_params = dict(input_shape=mdp.info.observation_space.shape,\n",
    "                           output_shape=(mdp.info.action_space.n,),\n",
    "                           n_actions = mdp.info.action_space.n,\n",
    "                           network = network,\n",
    "                           optimizer=optimizer,\n",
    "                           loss=F.smooth_l1_loss\n",
    "                           )\n",
    "approximator = TorchApproximator\n",
    "\n",
    "algorithm_params = dict(\n",
    "    batch_size=32,\n",
    "    target_update_frequency=target_update_frequency // train_frequency,\n",
    "    replay_memory=None,\n",
    "    initial_replay_size=initial_replay_size,\n",
    "    max_replay_size=max_replay_size\n",
    ")\n",
    "\n",
    "# Agent\n",
    "agent = DQN(mdp.info, pi, approximator,\n",
    "            approximator_params=approximator_params,**algorithm_params)\n",
    "core = Core(agent, mdp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################\n",
      "Epoch:  0\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 108\n",
      "################################################################\n",
      "Epoch:  1\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 9\n",
      "################################################################\n",
      "Epoch:  2\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 7\n",
      "################################################################\n",
      "Epoch:  3\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 4\n",
      "################################################################\n",
      "Epoch:  4\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 7\n",
      "################################################################\n",
      "Epoch:  5\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 8\n",
      "################################################################\n",
      "Epoch:  6\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 11\n",
      "################################################################\n",
      "Epoch:  7\n",
      "----------------------------------------------------------------\n",
      "- Learning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_reward: -1.000000, max_reward: -1.000000, mean_reward: -1.000000, games_completed: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Fill replay memory with random dataset\n",
    "print_epoch(0)\n",
    "core.learn(n_steps=initial_replay_size,\n",
    "           n_steps_per_fit=initial_replay_size)\n",
    "\n",
    "# Evaluate initial policy\n",
    "pi.set_epsilon(epsilon_test)\n",
    "dataset = core.evaluate(n_steps=test_samples)\n",
    "scores.append(get_stats(dataset))\n",
    "\n",
    "for n_epoch in range(1, max_steps // evaluation_frequency + 1):\n",
    "    print_epoch(n_epoch)\n",
    "    print('- Learning:')\n",
    "    # learning step\n",
    "    pi.set_epsilon(epsilon)\n",
    "    core.learn(n_steps=evaluation_frequency,\n",
    "               n_steps_per_fit=train_frequency)\n",
    "\n",
    "    print('- Evaluation:')\n",
    "    # evaluation step\n",
    "    pi.set_epsilon(epsilon_test)\n",
    "    dataset = core.evaluate(n_steps=test_samples)\n",
    "    scores.append(get_stats(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "dataset = core.evaluate(n_steps=3000, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('experiences.txt', 'w')\n",
    "for i in dataset:\n",
    "    f.write(''+('%s, %s, %s, %s, %s, %s' % (i[0], i[1], i[2],i[3], i[4],i[5]))+';\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c =zip(*dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r, ns, ab, d = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = zip(s, a, r, ns, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(c):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
